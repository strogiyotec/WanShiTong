# Quiz 1 

## Abstract text mining
Today, most information from different fields are stored in electonic documents(1, Mostafa,2018). Those documents contain structured(for example author , title, date of publication) and unstructured (for example Abstract paragraph)text components(1,Mostafa,2018). According to Mostafa, Text mining became "one of the trendy ﬁelds" which is widely used in different research areas such as "computational linguistics, Information Retrieval (IR) and data mining". However, text mining is different from data mining. As stated by Mostafa, the primar goal of data mining is to 
find useful patterns from given dataset. However, the biggest issue is that patterns produced from data mining aren't useful in case when users search information by unknown tearms. 
Here is when text mining comes in place. According to Mustafa, the main goal of text mining is find patterns from data "which is not indicated explicitly nor written down so far". In other words , text mining helps you to predict patterns from unknown strctures. An example of data that can be handled by text mining is Text documents or Html pages from web(1, Mustafa,2018).
According to Mustafa, the general algorithms for text mining are working according to following steps
1. Gathering documents
2. Pass all the documents through text analysis tool
3. Store the result from step two in management system(such as database)

Eventhough, data mining and text mining are used for different purposes, they both can use the same techniques to generate an information(1,Mustafa,2018).
According to Mustafa, "text mining is an interrelated ﬁeld with Natural Language Processing". Natural Language Processing(NLP) is a field that finds patterns between interrelation texts from unstructured documents(1, Mustafa,2018).

## Choosen topic - Text Generation
### Introduction
According to Wenhao(2, Wenhao,2020) the main goal of text generation is "machines express in human language". It's one of the hardest field on Natural Language Processing. The simplest implementation takes a text from a textual document and generat output text undertandable for humans. This algorithm is usually described as encoder-decoder schema where encoder means given text and decoder means output text(2,Wenhao,2020).

### What are the tools used for the area you chose.
The most popular technology for text generation is GPT-3. GPT-3 was created by a company called OpenAI. According to their website(3, OpenAI website) "OpenAI is an AI research and deployment company. Our mission is to ensure that artificial general intelligence benefits all of humanity.". One of the main goal of GPT-3 is to 
analyze patterns in a given dataset and produce new text with similar statistical patterns(4, Micaela, 2021). This technology is based on two technologies GPT and GPT-2. They all use the technology called Transformers(5, Tiernan,2020). Transformer takes a text and uses a function called "attention" to predict a likelyhood that a word will appear based on a given set of words(5, Tiernan,2020). For comparison, GPT-2 was based on 1.5 billion parameters while GPT-3 is based on 175 billion(5, Tiernan, 2020). According to Tiernan, parameter is a calculation that gives a weight score to the data (less or great), this score then will be used int the overall calculation. GPT-3 is able to perform meta-learning which means that algorithm is not re-trained to perform sentence completion(5, Tiernan,2020). There are however some shortcomings. GPT-3 can't achieve an accuracy in Adversarial NLI(5, Tiernan,2020). According to Tiernan, Adversarial NLI is a task of detecting relationships between two different sentences. As said by Tiernan, in this case , GPT-3 performse "little better than chance".

### What are the applications of the are of text mining you chose
GPT-3 has an enormous amount of use cases. According to OpenAI website(3,OpenAI), GPT-3 is used 
1. Reddit to support chat bot functionality
2. Code Refactoring to transpile source code of one language to another
3. Summorization to write a summary for books

Guardian wrote the whole article using GPT-3 in order to show the capability of this technology (6, Guardian, 2020).


https://www.youtube.com/watch?v=SY5PvZrJhLE
https://thenextweb.com/news/this-gpt-3-powered-tool-generates-new-ideas-for-your-terrible-blog
https://www.wired.com/story/ai-text-generator-gpt-3-learning-language-fitfully/

## References
1. Mostafa Al-Emran, Using Text Mining Techniques for Extracting Information from Research Articles, 2018 - https://www.researchgate.net/publication/321150349_Using_Text_Mining_Techniques_for_Extracting_Information_from_Research_Articles
2. Wenhao Yu, A Survey of Knowledge-Enhanced Text Generation, 2020 - https://www.microsoft.com/en-us/research/uploads/prod/2020/10/2010.04389.pdf
3. OpenAI website - https://openai.com/about/
4. Micaela Simeone, The Scariest Deepfake of All: AI Text Generator GPT-3, 2021- https://students.bowdoin.edu/bowdoin-science-journal/science/the-scariest-deepfake-of-all-ai-text-generator-gpt-3/
5. Tiernan Ray , OpenAI’s gigantic GPT-3 hints at the limits of language models for AI, 2020 - https://www.zdnet.com/article/openais-gigantic-gpt-3-hints-at-the-limits-of-language-models-for-ai/
6. Guardian ,A robot wrote this entire article. Are you scared yet, human, 2020 - https://www.theguardian.com/commentisfree/2020/sep/08/robot-wrote-this-article-gpt-3
